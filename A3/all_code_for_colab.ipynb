{"cells":[{"cell_type":"code","metadata":{"id":"VPMqZ9WdClCg","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpmMvEsf_ELc","colab_type":"code","outputId":"5575a20c-6427-4d8c-cb6c-8ce66871d998","executionInfo":{"status":"ok","timestamp":1587720126050,"user_tz":-120,"elapsed":2764,"user":{"displayName":"R Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFagftgYSL9BrqaggjFXP5dcTT5-nMKeSRy5kY=s64","userId":"01507122395908804271"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["!/opt/bin/nvidia-smi"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Fri Apr 24 09:22:05 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wL579x0O6E19","colab_type":"code","colab":{}},"source":["\"\"\"\n","Wrappers in this cell are from Open AI:\n","https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n","\"\"\"\n","\n","import numpy as np\n","from collections import deque\n","import gym\n","from gym import spaces\n","import cv2\n","\n","\n","class NoopResetEnv(gym.Wrapper):\n","    def __init__(self, env=None, noop_max=30):\n","        \"\"\"Sample initial states by taking random number of no-ops on reset.\n","        No-op is assumed to be action 0.\n","        \"\"\"\n","        super(NoopResetEnv, self).__init__(env)\n","        self.noop_max = noop_max\n","        self.override_num_noops = None\n","        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n","\n","    def reset(self):\n","        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n","        self.env.reset()\n","        if self.override_num_noops is not None:\n","            noops = self.override_num_noops\n","        else:\n","            noops = np.random.randint(1, self.noop_max + 1)\n","        assert noops > 0\n","        obs = None\n","        for _ in range(noops):\n","            obs, _, done, _ = self.env.step(0)\n","            if done:\n","                obs = self.env.reset()\n","        return obs\n","\n","\n","class FireResetEnv(gym.Wrapper):\n","    def __init__(self, env=None):\n","        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n","        super(FireResetEnv, self).__init__(env)\n","        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n","        assert len(env.unwrapped.get_action_meanings()) >= 3\n","\n","    def reset(self):\n","        self.env.reset()\n","        obs, _, done, _ = self.env.step(1)\n","        if done:\n","            self.env.reset()\n","        obs, _, done, _ = self.env.step(2)\n","        if done:\n","            self.env.reset()\n","        return obs\n","\n","\n","class EpisodicLifeEnv(gym.Wrapper):\n","    def __init__(self, env=None):\n","        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n","        Done by DeepMind for the DQN and co. since it helps value estimation.\n","        \"\"\"\n","        super(EpisodicLifeEnv, self).__init__(env)\n","        self.lives = 0\n","        self.was_real_done = True\n","        self.was_real_reset = False\n","\n","    def step(self, action):\n","        obs, reward, done, info = self.env.step(action)\n","        self.was_real_done = done\n","        # check current lives, make loss of life terminal,\n","        # then update lives to handle bonus lives\n","        lives = self.env.unwrapped.ale.lives()\n","        if lives < self.lives and lives > 0:\n","            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n","            # so its important to keep lives > 0, so that we only reset once\n","            # the environment advertises done.\n","            done = True\n","        self.lives = lives\n","        return obs, reward, done, info\n","\n","    def reset(self):\n","        \"\"\"Reset only when lives are exhausted.\n","        This way all states are still reachable even though lives are episodic,\n","        and the learner need not know about any of this behind-the-scenes.\n","        \"\"\"\n","        if self.was_real_done:\n","            obs = self.env.reset()\n","            self.was_real_reset = True\n","        else:\n","            # no-op step to advance from terminal/lost life state\n","            obs, _, _, _ = self.env.step(0)\n","            self.was_real_reset = False\n","        self.lives = self.env.unwrapped.ale.lives()\n","        return obs\n","\n","\n","class ProcessFrame84(gym.ObservationWrapper):\n","    def __init__(self, env=None):\n","        super(ProcessFrame84, self).__init__(env)\n","        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n","\n","    def observation(self, obs):\n","        return ProcessFrame84.process(obs)\n","\n","    @staticmethod\n","    def process(frame):\n","        if frame.size == 210 * 160 * 3:\n","            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n","        elif frame.size == 250 * 160 * 3:\n","            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n","        else:\n","            assert False, \"Unknown resolution.\"\n","        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n","        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n","        x_t = resized_screen[18:102, :]\n","        x_t = np.reshape(x_t, [84, 84, 1])\n","        return x_t.astype(np.uint8)\n","\n","\n","class ClippedRewardsWrapper(gym.RewardWrapper):\n","    def reward(self, reward):\n","        \"\"\"Change all the positive rewards to 1, negative to -1 and keep zero.\"\"\"\n","        return np.sign(reward)\n","\n","\n","class LazyFrames(object):\n","    def __init__(self, frames):\n","        \"\"\"This object ensures that common frames between the observations are only stored once.\n","        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n","        buffers.\n","        This object should only be converted to numpy array before being passed to the model.\n","        You'd not belive how complex the previous solution was.\"\"\"\n","        self._frames = frames\n","\n","    def __array__(self, dtype=None):\n","        out = np.concatenate(self._frames, axis=0)\n","        if dtype is not None:\n","            out = out.astype(dtype)\n","        return out\n","\n","\n","class FrameStack(gym.Wrapper):\n","    def __init__(self, env, k):\n","        \"\"\"Stack k last frames.\n","        Returns lazy array, which is much more memory efficient.\n","        See Also\n","        --------\n","        baselines.common.atari_wrappers.LazyFrames\n","        \"\"\"\n","        gym.Wrapper.__init__(self, env)\n","        self.k = k\n","        self.frames = deque([], maxlen=k)\n","        shp = env.observation_space.shape\n","        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]))\n","\n","    def reset(self):\n","        ob = self.env.reset()\n","        for _ in range(self.k):\n","            self.frames.append(ob)\n","        return self._get_ob()\n","\n","    def step(self, action):\n","        ob, reward, done, info = self.env.step(action)\n","        self.frames.append(ob)\n","        return self._get_ob(), reward, done, info\n","\n","    def _get_ob(self):\n","        assert len(self.frames) == self.k\n","        return LazyFrames(list(self.frames))\n","\n","\n","class ChannelsFirstImageShape(gym.ObservationWrapper):\n","    \"\"\"\n","    Change image shape to CWH\n","    \"\"\"\n","    def __init__(self, env):\n","        super(ChannelsFirstImageShape, self).__init__(env)\n","        old_shape = self.observation_space.shape\n","        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n","\n","    def observation(self, observation):\n","        return np.swapaxes(observation, 2, 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoNX3gxkhD-J","colab_type":"code","colab":{}},"source":["class Environment():\n","    def __init__(self, env_name, is_video=False):\n","        self.env = gym.make(env_name)\n","        self.action_space = self.env.action_space\n","        self.observation_space = self.env.observation_space\n","        if is_video:\n","            self.env = gym.wrappers.Monitor(self.env, \"./output/video/\", force=True)\n","\n","    def gen_wrapped_env(self):\n","        self.env = NoopResetEnv(self.env, noop_max=30)\n","        if 'FIRE' in self.env.unwrapped.get_action_meanings():\n","            self.env = FireResetEnv(self.env)\n","        self.env = ProcessFrame84(self.env)\n","        self.env = ChannelsFirstImageShape(self.env)\n","        self.env = FrameStack(self.env, 4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ee1NBzin4PQY","colab_type":"text"},"source":["# Wrapper\n","----\n","# Logger"]},{"cell_type":"code","metadata":{"id":"yPESR_L63QkG","colab_type":"code","colab":{}},"source":["import os\n","import csv\n","import numpy as np\n","import shutil\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","import datetime\n","\n","TRAINING_UPDATE_FREQUENCY = 1000\n","RUN_UPDATE_FREQUENCY = 10\n","\n","\n","class Logger():\n","\n","    def __init__(self, header, directory_path):\n","        self.header = header\n","        self.directory_path = directory_path + self.timestamp() + \"/\"\n","\n","        self.score = []\n","        self.step = []\n","        self.loss = []\n","        self.accuracy = []\n","        self.q = []\n","\n","        if os.path.exists(self.directory_path):\n","            shutil.rmtree(self.directory_path, ignore_errors=True)\n","        os.makedirs(self.directory_path)\n","\n","    def timestamp(self):\n","        return str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n","\n","    def add_gameover(self, gameover):\n","        if gameover % RUN_UPDATE_FREQUENCY == 0:\n","            print('{{\"metric\": \"gameover\", \"value\": {}}}'.format(gameover))\n","\n","    def add_score(self, value):\n","        self.add_log(self.score, value, \"gameover\", \"score\", RUN_UPDATE_FREQUENCY, self.directory_path, self.header)\n","\n","    def add_step(self, value):\n","        self.add_log(self.step, value, \"gameover\", \"number of steps\", RUN_UPDATE_FREQUENCY, self.directory_path, self.header)\n","\n","    def add_accuracy(self, value):\n","        self.add_log(self.accuracy, value, \"update\", \"accuracy\", TRAINING_UPDATE_FREQUENCY, self.directory_path, self.header)\n","\n","    def add_loss(self, value):\n","        value = min(5, value)  # clip loss, max = 5\n","        self.add_log(self.loss, value, \"update\", \"loss\", TRAINING_UPDATE_FREQUENCY, self.directory_path, self.header)\n","\n","    # def add_q(self, value):\n","    #     self.q.add_log(self.q, value, \"update\", \"q\", TRAINING_UPDATE_FREQUENCY, self.directory_path, self.header)\n","\n","    def add_log(self, values, value, x_label, y_label, update_frequency, directory_path, header):\n","        values.append(value)\n","        if len(values) % update_frequency == 0:\n","            mean_value = np.mean(values)\n","            print(y_label + \": (min: \" + str(min(values)) + \", avg: \" + str(mean_value) + \", max: \" + str(max(values)))\n","            print('{\"metric\": \"' + y_label + '\", \"value\": {}}}'.format(mean_value))\n","            self.save_data(self.directory_path + y_label + \".csv\", mean_value)\n","            self.save_fig(input_path=self.directory_path + y_label + \".csv\",\n","                           output_path=self.directory_path + y_label + \".png\",\n","                           small_batch_length=update_frequency,\n","                           big_batch_length=update_frequency*10,\n","                           x_label=x_label,\n","                           y_label=y_label)\n","            values = []\n","\n","    def save_fig(self, input_path, output_path, small_batch_length, big_batch_length, x_label, y_label):\n","        x = []\n","        y = []\n","        with open(input_path, \"r\") as scores:\n","            reader = csv.reader(scores)\n","            data = list(reader)\n","            for i in range(0, len(data)):\n","                x.append(float(i)*small_batch_length)\n","                y.append(float(data[i][0]))\n","\n","        plt.subplots()\n","        plt.plot(x, y, label=\"last \" + str(small_batch_length) + \" average\")\n","\n","        plt.title(self.header)\n","        plt.xlabel(x_label)\n","        plt.ylabel(y_label)\n","        plt.legend(loc=\"upper left\")\n","        plt.savefig(output_path, bbox_inches=\"tight\")\n","        plt.close()\n","\n","    def save_data(self, path, score):\n","        if not os.path.exists(path):\n","            with open(path, \"w\"):\n","                pass\n","        scores_file = open(path, \"a\")\n","        with scores_file:\n","            writer = csv.writer(scores_file)\n","            writer.writerow([score])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyc_6lDx4SPj","colab_type":"text"},"source":["# Logger\n","-----\n","# Network"]},{"cell_type":"code","metadata":{"id":"-_ddnwrry8rk","colab_type":"code","colab":{}},"source":["from keras.optimizers import RMSprop\n","from keras.models import Sequential\n","from keras.layers import Conv2D, Flatten, Dense\n","\n","\n","class CNN():\n","    def __init__(self, input_shape, action_space, is_dueling=False):\n","        self.input_shape = input_shape\n","        self.action_space = action_space\n","        self.optimizer = RMSprop(lr=0.00025,rho=0.95,epsilon=0.01)\n","        self.model = self.build_model()\n","\n","    def build_model(self):\n","        model = Sequential()\n","        model.add(Conv2D(32,8,\n","                    strides=(4, 4),\n","                    padding=\"valid\",\n","                    activation=\"relu\",\n","                    input_shape=self.input_shape,\n","                    data_format=\"channels_first\"))\n","        model.add(Conv2D(64,4,\n","                    strides=(2, 2),\n","                    padding=\"valid\",\n","                    activation=\"relu\",\n","                    input_shape=self.input_shape,\n","                    data_format=\"channels_first\"))\n","        model.add(Conv2D(64,3,\n","                    strides=(1, 1),\n","                    padding=\"valid\",\n","                    activation=\"relu\",\n","                    input_shape=self.input_shape,\n","                    data_format=\"channels_first\"))\n","        model.add(Flatten())\n","        model.add(Dense(512, activation=\"relu\"))\n","        model.add(Dense(self.action_space))\n","        model.compile(loss=\"mse\",\n","                  optimizer=self.optimizer,\n","                  metrics=[\"accuracy\"])\n","        model.summary()\n","        return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QGI_yXtb4caR","colab_type":"text"},"source":["# Network\n","----\n","# Model"]},{"cell_type":"code","metadata":{"id":"bQM_aJGP4K4N","colab_type":"code","colab":{}},"source":["import numpy as np\n","import os\n","import random\n","import shutil\n","\n","\n","GAMMA = 0.99\n","MEMORY_SIZE = 900000\n","BATCH_SIZE = 32\n","TRAINING_FREQUENCY = 4\n","TARGET_NETWORK_UPDATE_FREQUENCY = 40000\n","MODEL_PERSISTENCE_UPDATE_FREQUENCY = 10000\n","REPLAY_START_SIZE = 50000\n","\n","EPSILON_MAX = 1.0\n","EPSILON_MIN = 0.1\n","EPSILON_TEST = 0.02\n","EPSILON_STEPS = 850000\n","\n","class ReplayMemory():\n","    def __init__(self, memory_size=1000000, batch_size=32):\n","      self.memory_size = memory_size\n","      self.batch_size = batch_size\n","      self.replays = []\n","\n","    def add_replay(self, state, action, reward, next_state, terminal):\n","        new_replay = {\"state\": state,\n","                \"action\": action,\n","                \"reward\": reward,\n","                \"next_state\": next_state,\n","                \"terminal\": terminal}\n","        self.replays.append(new_replay)\n","        if len(self.replays) > self.memory_size:\n","            self.replays.pop(0)\n","\n","    def gen_batch(self):\n","        batch = np.asarray(random.sample(self.replays, self.batch_size))\n","        return batch\n","\n","\n","class DQNTrain():\n","\n","    def __init__(self, game_name, input_shape, action_space, save_log=True, save_weights=True):\n","        self.action_space = action_space\n","        self.input_shape = input_shape\n","        self.epsilon = EPSILON_MAX\n","        self.memory = ReplayMemory(MEMORY_SIZE, BATCH_SIZE)\n","\n","        self.ddqn_eval = CNN(self.input_shape, self.action_space).model\n","        self.ddqn_target = CNN(self.input_shape, self.action_space).model\n","        self.update_target_weights()\n","\n","        self.save_log = save_log\n","        self.save_weights = save_weights\n","        self.save_path = \"./output/train\"\n","        self.logger = Logger(game_name + \"Train\", self.save_path + \"/logs/ddqn/\")\n","\n","    def get_action(self, state):\n","        if np.random.uniform() < self.epsilon or len(self.memory.replays) < REPLAY_START_SIZE:\n","            return random.randrange(self.action_space)\n","        else:\n","            q_values = self.ddqn_eval.predict(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n","            return np.argmax(q_values[0])\n","\n","    def update_epsilon(self):\n","        self.epsilon -= (EPSILON_MAX-EPSILON_MIN)/EPSILON_STEPS\n","        if self.epsilon < EPSILON_MIN:\n","          self.epsilon = EPSILON_MIN \n","\n","    def save_model(self):\n","        model_path = self.save_path + \"/models/ddqn/\" + self.logger.timestamp() + \"/model.h5\"\n","        if os.path.exists(os.path.dirname(model_path)):\n","            shutil.rmtree(os.path.dirname(model_path), ignore_errors=True)\n","        os.makedirs(os.path.dirname(model_path))\n","        self.ddqn_eval.save_weights(model_path)\n","\n","    def update_target_weights(self):\n","        self.ddqn_target.set_weights(self.ddqn_eval.get_weights())\n","\n","    def update(self, total_step):\n","        if len(self.memory.replays) < REPLAY_START_SIZE:\n","            return\n","\n","        if total_step % TRAINING_FREQUENCY == 0 and self.save_log:\n","            loss, accuracy = self.learn()\n","            self.log_model_status(loss, accuracy)\n","    \n","        self.update_epsilon()\n","\n","        if total_step % MODEL_PERSISTENCE_UPDATE_FREQUENCY == 0 and self.save_weights:\n","            self.save_model()\n","\n","        if total_step % TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n","            self.update_target_weights()\n","\n","    def learn(self, mode=2015):\n","        batch = self.memory.gen_batch()\n","\n","        states = []\n","        q_values = []\n","        # max_q_values = []  TODELETE\n","\n","        for record in batch:\n","            state = np.expand_dims(np.asarray(record[\"state\"]).astype(np.float64), axis=0)\n","            states.append(state)\n","            next_state = np.expand_dims(np.asarray(record[\"next_state\"]).astype(np.float64), axis=0)\n","\n","            q = list(self.ddqn_eval.predict(state)[0])\n","            if mode == 2015:\n","               q[record[\"action\"]] = record[\"reward\"] + (1 - record[\"terminal\"]) * GAMMA * np.max(self.ddqn_target.predict(next_state).ravel())\n","            elif mode == 2013:\n","                q[record[\"action\"]] = record[\"reward\"] + (1 - record[\"terminal\"]) * GAMMA * np.max(self.ddqn.predict(next_state).ravel())\n","            elif mode == 2016:\n","                next_max_action = np.argmax(self.ddqn.predict(next_state).ravel())\n","                q[record[\"action\"]] = record[\"reward\"] + (1 - record[\"terminal\"]) * GAMMA * self.ddqn_target.predict(next_state).ravel()[next_max_action]\n","\n","            q_values.append(q)\n","            # max_q_values.append(np.max(q))\n","\n","        fit = self.ddqn_eval.fit(np.asarray(states).squeeze(),\n","                      np.asarray(q_values).squeeze(),\n","                      batch_size=BATCH_SIZE,\n","                      verbose=0)\n","        \n","        loss = fit.history[\"loss\"][0]\n","        accuracy = fit.history[\"accuracy\"][0]\n","        # return loss, accuracy, np.mean(max_q_values)  TODELETE\n","        return loss, accuracy\n","\n","    def log_game_status(self, score, step, gameover):\n","        self.logger.add_score(score)\n","        self.logger.add_step(step)\n","        self.logger.add_gameover(gameover)\n","\n","    def log_model_status(self, loss, accuracy):\n","        self.logger.add_loss(loss)\n","        self.logger.add_accuracy(accuracy)\n","        # self.logger.add_q(average_max_q)  TODELETE\n","\n","\n","class DQNTest():\n","    def __init__(self, game_name, input_shape, action_space, testing_model_path):\n","        self.action_space = action_space\n","        self.ddqn = CNN(input_shape, action_space).model\n","        self.logger = Logger(game_name + \"Test\", \"./output/test/logs/ddqn/\")\n","        assert os.path.exists(os.path.dirname(testing_model_path)), \"No testing model in: \" + str(testing_model_path)\n","        if os.path.isfile(testing_model_path):\n","            self.ddqn.load_weights(testing_model_path)\n","\n","    def get_action(self, state):\n","        if np.random.rand() < EPSILON_TEST:\n","            return random.randrange(self.action_space)\n","        q_values = self.ddqn.predict(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n","        return np.argmax(q_values[0])\n","\n","    def log_model_status(self, loss, accuracy):\n","        self.logger.add_loss(loss)\n","        self.logger.add_accuracy(accuracy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fp4kfSp76S6n","colab_type":"text"},"source":["# Model\n","----\n","# Entry"]},{"cell_type":"code","metadata":{"id":"6uYvL6IZ6SEB","colab_type":"code","outputId":"a3dfc7a2-68ba-45f4-960d-d395e7fa7bf5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import gym\n","import argparse\n","import numpy as np\n","import atari_py\n","\n","# preprocessing & generate env\n","game_name = \"Breakout\"\n","env_name = game_name + \"Deterministic-v4\"  \n","breakout_env = Environment(env_name)\n","breakout_env.gen_wrapped_env()\n","env = breakout_env.env\n","action_space = env.action_space.n\n","img_size = 84\n","input_shape = (action_space, img_size, img_size)\n","max_train_step = 2000000\n","\n","# generate model\n","is_train = True\n","model = DQNTrain(game_name, input_shape, action_space) if is_train else DQNTest(game_name, input_shape, action_space)\n"," \n","# iteration of updating the Q table\n","gameover = 0\n","total_step = 0\n","while True:\n","    step = 0\n","    score = 0\n","    state = env.reset()\n","    while True:\n","        if total_step >= max_train_step:\n","            print(f\"Traning of {max_train_step} steps completed!\")\n","            exit(0)\n","        action = model.get_action(state)\n","        next_state, reward, terminal, info = env.step(action)\n","        reward = np.sign(reward)\n","        if is_train:\n","          model.memory.add_replay(state, action, reward, next_state, terminal)\n","\n","        score += reward\n","        state = next_state\n","        total_step += 1\n","        if is_train:\n","          model.update(total_step)\n","        step += 1\n","        \n","        if terminal:\n","            model.log_game_status(score, step, gameover)\n","            gameover += 1\n","            break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_7 (Conv2D)            (None, 32, 20, 20)        8224      \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 64, 9, 9)          32832     \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 64, 7, 7)          36928     \n","_________________________________________________________________\n","flatten_3 (Flatten)          (None, 3136)              0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 512)               1606144   \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 4)                 2052      \n","=================================================================\n","Total params: 1,686,180\n","Trainable params: 1,686,180\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_10 (Conv2D)           (None, 32, 20, 20)        8224      \n","_________________________________________________________________\n","conv2d_11 (Conv2D)           (None, 64, 9, 9)          32832     \n","_________________________________________________________________\n","conv2d_12 (Conv2D)           (None, 64, 7, 7)          36928     \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 3136)              0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 512)               1606144   \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 4)                 2052      \n","=================================================================\n","Total params: 1,686,180\n","Trainable params: 1,686,180\n","Non-trainable params: 0\n","_________________________________________________________________\n","{\"metric\": \"gameover\", \"value\": 0}\n","score: (min: 0.0, avg: 1.0, max: 3.0\n","{\"metric\": \"score\", \"value\": 1.0}\n","number of steps: (min: 121, avg: 173.5, max: 275\n","{\"metric\": \"number of steps\", \"value\": 173.5}\n","{\"metric\": \"gameover\", \"value\": 10}\n","score: (min: 0.0, avg: 1.4, max: 4.0\n","{\"metric\": \"score\", \"value\": 1.4}\n","number of steps: (min: 121, avg: 188.0, max: 324\n","{\"metric\": \"number of steps\", \"value\": 188.0}\n","{\"metric\": \"gameover\", \"value\": 20}\n","score: (min: 0.0, avg: 1.3, max: 4.0\n","{\"metric\": \"score\", \"value\": 1.3}\n","number of steps: (min: 121, avg: 183.53333333333333, max: 324\n","{\"metric\": \"number of steps\", \"value\": 183.53333333333333}\n","{\"metric\": \"gameover\", \"value\": 30}\n","score: (min: 0.0, avg: 1.075, max: 4.0\n","{\"metric\": \"score\", \"value\": 1.075}\n","number of steps: (min: 121, avg: 175.425, max: 324\n","{\"metric\": \"number of steps\", \"value\": 175.425}\n","{\"metric\": \"gameover\", \"value\": 40}\n","score: (min: 0.0, avg: 1.12, max: 4.0\n","{\"metric\": \"score\", \"value\": 1.12}\n","number of steps: (min: 121, avg: 176.54, max: 324\n","{\"metric\": \"number of steps\", \"value\": 176.54}\n","{\"metric\": \"gameover\", \"value\": 50}\n","score: (min: 0.0, avg: 1.1833333333333333, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1833333333333333}\n","number of steps: (min: 121, avg: 179.13333333333333, max: 345\n","{\"metric\": \"number of steps\", \"value\": 179.13333333333333}\n","{\"metric\": \"gameover\", \"value\": 60}\n","score: (min: 0.0, avg: 1.1571428571428573, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1571428571428573}\n","number of steps: (min: 121, avg: 177.85714285714286, max: 345\n","{\"metric\": \"number of steps\", \"value\": 177.85714285714286}\n","{\"metric\": \"gameover\", \"value\": 70}\n","score: (min: 0.0, avg: 1.05, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.05}\n","number of steps: (min: 121, avg: 173.425, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.425}\n","{\"metric\": \"gameover\", \"value\": 80}\n","score: (min: 0.0, avg: 1.0666666666666667, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0666666666666667}\n","number of steps: (min: 121, avg: 173.37777777777777, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.37777777777777}\n","{\"metric\": \"gameover\", \"value\": 90}\n","score: (min: 0.0, avg: 1.07, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.07}\n","number of steps: (min: 121, avg: 173.54, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.54}\n","{\"metric\": \"gameover\", \"value\": 100}\n","score: (min: 0.0, avg: 1.0818181818181818, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0818181818181818}\n","number of steps: (min: 121, avg: 173.37272727272727, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.37272727272727}\n","{\"metric\": \"gameover\", \"value\": 110}\n","score: (min: 0.0, avg: 1.0916666666666666, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0916666666666666}\n","number of steps: (min: 121, avg: 173.825, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.825}\n","{\"metric\": \"gameover\", \"value\": 120}\n","score: (min: 0.0, avg: 1.0846153846153845, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0846153846153845}\n","number of steps: (min: 121, avg: 173.15384615384616, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.15384615384616}\n","{\"metric\": \"gameover\", \"value\": 130}\n","score: (min: 0.0, avg: 1.0857142857142856, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0857142857142856}\n","number of steps: (min: 121, avg: 173.47857142857143, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.47857142857143}\n","{\"metric\": \"gameover\", \"value\": 140}\n","score: (min: 0.0, avg: 1.0866666666666667, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0866666666666667}\n","number of steps: (min: 121, avg: 173.43333333333334, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.43333333333334}\n","{\"metric\": \"gameover\", \"value\": 150}\n","score: (min: 0.0, avg: 1.0875, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0875}\n","number of steps: (min: 121, avg: 173.375, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.375}\n","{\"metric\": \"gameover\", \"value\": 160}\n","score: (min: 0.0, avg: 1.0823529411764705, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0823529411764705}\n","number of steps: (min: 121, avg: 173.16470588235293, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.16470588235293}\n","{\"metric\": \"gameover\", \"value\": 170}\n","score: (min: 0.0, avg: 1.1166666666666667, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1166666666666667}\n","number of steps: (min: 121, avg: 174.59444444444443, max: 345\n","{\"metric\": \"number of steps\", \"value\": 174.59444444444443}\n","{\"metric\": \"gameover\", \"value\": 180}\n","score: (min: 0.0, avg: 1.1, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1}\n","number of steps: (min: 121, avg: 173.95263157894738, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.95263157894738}\n","{\"metric\": \"gameover\", \"value\": 190}\n","score: (min: 0.0, avg: 1.115, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.115}\n","number of steps: (min: 121, avg: 174.5, max: 345\n","{\"metric\": \"number of steps\", \"value\": 174.5}\n","{\"metric\": \"gameover\", \"value\": 200}\n","score: (min: 0.0, avg: 1.1142857142857143, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1142857142857143}\n","number of steps: (min: 121, avg: 174.5142857142857, max: 345\n","{\"metric\": \"number of steps\", \"value\": 174.5142857142857}\n","{\"metric\": \"gameover\", \"value\": 210}\n","score: (min: 0.0, avg: 1.1272727272727272, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1272727272727272}\n","number of steps: (min: 121, avg: 175.00454545454545, max: 345\n","{\"metric\": \"number of steps\", \"value\": 175.00454545454545}\n","{\"metric\": \"gameover\", \"value\": 220}\n","score: (min: 0.0, avg: 1.1130434782608696, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1130434782608696}\n","number of steps: (min: 121, avg: 174.18695652173912, max: 345\n","{\"metric\": \"number of steps\", \"value\": 174.18695652173912}\n","{\"metric\": \"gameover\", \"value\": 230}\n","score: (min: 0.0, avg: 1.0958333333333334, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0958333333333334}\n","number of steps: (min: 121, avg: 173.67916666666667, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.67916666666667}\n","{\"metric\": \"gameover\", \"value\": 240}\n","score: (min: 0.0, avg: 1.104, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.104}\n","number of steps: (min: 121, avg: 173.632, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.632}\n","{\"metric\": \"gameover\", \"value\": 250}\n","score: (min: 0.0, avg: 1.0846153846153845, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0846153846153845}\n","number of steps: (min: 121, avg: 173.13076923076923, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.13076923076923}\n","{\"metric\": \"gameover\", \"value\": 260}\n","score: (min: 0.0, avg: 1.0925925925925926, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0925925925925926}\n","number of steps: (min: 121, avg: 173.47037037037038, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.47037037037038}\n","{\"metric\": \"gameover\", \"value\": 270}\n","score: (min: 0.0, avg: 1.1, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1}\n","number of steps: (min: 121, avg: 173.675, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.675}\n","{\"metric\": \"gameover\", \"value\": 280}\n","score: (min: 0.0, avg: 1.096551724137931, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.096551724137931}\n","number of steps: (min: 121, avg: 173.69655172413792, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.69655172413792}\n","{\"metric\": \"gameover\", \"value\": 290}\n","score: (min: 0.0, avg: 1.1233333333333333, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.1233333333333333}\n","number of steps: (min: 121, avg: 174.73, max: 345\n","{\"metric\": \"number of steps\", \"value\": 174.73}\n","{\"metric\": \"gameover\", \"value\": 300}\n","score: (min: 0.0, avg: 1.096774193548387, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.096774193548387}\n","number of steps: (min: 121, avg: 173.76774193548388, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.76774193548388}\n","loss: (min: 0.012088056653738022, avg: 0.7005803831480444, max: 5\n","{\"metric\": \"loss\", \"value\": 0.7005803831480444}\n","accuracy: (min: 0.03125, avg: 0.5611563, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5611562728881836}\n","{\"metric\": \"gameover\", \"value\": 310}\n","score: (min: 0.0, avg: 1.084375, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.084375}\n","number of steps: (min: 121, avg: 173.325, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.325}\n","{\"metric\": \"gameover\", \"value\": 320}\n","score: (min: 0.0, avg: 1.0696969696969696, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0696969696969696}\n","number of steps: (min: 121, avg: 172.6969696969697, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.6969696969697}\n","{\"metric\": \"gameover\", \"value\": 330}\n","loss: (min: 0.00874984823167324, avg: 0.537649467961397, max: 5\n","{\"metric\": \"loss\", \"value\": 0.537649467961397}\n","accuracy: (min: 0.03125, avg: 0.55625, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5562499761581421}\n","score: (min: 0.0, avg: 1.0705882352941176, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0705882352941176}\n","number of steps: (min: 121, avg: 172.64705882352942, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.64705882352942}\n","{\"metric\": \"gameover\", \"value\": 340}\n","score: (min: 0.0, avg: 1.0885714285714285, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0885714285714285}\n","number of steps: (min: 121, avg: 173.35142857142858, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.35142857142858}\n","{\"metric\": \"gameover\", \"value\": 350}\n","loss: (min: 0.007399651221930981, avg: 0.4720816108509898, max: 5\n","{\"metric\": \"loss\", \"value\": 0.4720816108509898}\n","accuracy: (min: 0.03125, avg: 0.55647916, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5564791560173035}\n","score: (min: 0.0, avg: 1.0888888888888888, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0888888888888888}\n","number of steps: (min: 121, avg: 173.67222222222222, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.67222222222222}\n","{\"metric\": \"gameover\", \"value\": 360}\n","score: (min: 0.0, avg: 1.0837837837837838, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0837837837837838}\n","number of steps: (min: 121, avg: 173.17567567567568, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.17567567567568}\n","{\"metric\": \"gameover\", \"value\": 370}\n","score: (min: 0.0, avg: 1.0789473684210527, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0789473684210527}\n","number of steps: (min: 121, avg: 172.8842105263158, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.8842105263158}\n","{\"metric\": \"gameover\", \"value\": 380}\n","loss: (min: 0.005781953223049641, avg: 0.4255623639149126, max: 5\n","{\"metric\": \"loss\", \"value\": 0.4255623639149126}\n","accuracy: (min: 0.03125, avg: 0.557875, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5578749775886536}\n","score: (min: 0.0, avg: 1.082051282051282, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.082051282051282}\n","number of steps: (min: 121, avg: 172.86410256410255, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.86410256410255}\n","{\"metric\": \"gameover\", \"value\": 390}\n","score: (min: 0.0, avg: 1.075, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.075}\n","number of steps: (min: 121, avg: 172.7475, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.7475}\n","{\"metric\": \"gameover\", \"value\": 400}\n","loss: (min: 0.005627972073853016, avg: 0.398153041504696, max: 5\n","{\"metric\": \"loss\", \"value\": 0.398153041504696}\n","accuracy: (min: 0.03125, avg: 0.56085, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5608500242233276}\n","score: (min: 0.0, avg: 1.0853658536585367, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0853658536585367}\n","number of steps: (min: 121, avg: 173.19268292682926, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.19268292682926}\n","{\"metric\": \"gameover\", \"value\": 410}\n","score: (min: 0.0, avg: 1.0904761904761904, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0904761904761904}\n","number of steps: (min: 121, avg: 173.36666666666667, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.36666666666667}\n","{\"metric\": \"gameover\", \"value\": 420}\n","loss: (min: 0.00515197915956378, avg: 0.3788667404424244, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3788667404424244}\n","accuracy: (min: 0.03125, avg: 0.5630052, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5630052089691162}\n","score: (min: 0.0, avg: 1.0953488372093023, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0953488372093023}\n","number of steps: (min: 121, avg: 173.45581395348836, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.45581395348836}\n","{\"metric\": \"gameover\", \"value\": 430}\n","score: (min: 0.0, avg: 1.0909090909090908, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0909090909090908}\n","number of steps: (min: 121, avg: 173.34545454545454, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.34545454545454}\n","{\"metric\": \"gameover\", \"value\": 440}\n","score: (min: 0.0, avg: 1.08, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.08}\n","number of steps: (min: 120, avg: 172.88222222222223, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.88222222222223}\n","loss: (min: 0.005017785355448723, avg: 0.36200786724859585, max: 5\n","{\"metric\": \"loss\", \"value\": 0.36200786724859585}\n","accuracy: (min: 0.03125, avg: 0.5667679, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5667678713798523}\n","{\"metric\": \"gameover\", \"value\": 450}\n","score: (min: 0.0, avg: 1.0869565217391304, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0869565217391304}\n","number of steps: (min: 120, avg: 173.12173913043478, max: 345\n","{\"metric\": \"number of steps\", \"value\": 173.12173913043478}\n","{\"metric\": \"gameover\", \"value\": 460}\n","score: (min: 0.0, avg: 1.078723404255319, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.078723404255319}\n","number of steps: (min: 120, avg: 172.8404255319149, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.8404255319149}\n","{\"metric\": \"gameover\", \"value\": 470}\n","loss: (min: 0.005017785355448723, avg: 0.3494097519995994, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3494097519995994}\n","accuracy: (min: 0.03125, avg: 0.57041013, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5704101324081421}\n","score: (min: 0.0, avg: 1.075, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.075}\n","number of steps: (min: 120, avg: 172.7375, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.7375}\n","{\"metric\": \"gameover\", \"value\": 480}\n","score: (min: 0.0, avg: 1.073469387755102, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.073469387755102}\n","number of steps: (min: 120, avg: 172.60204081632654, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.60204081632654}\n","{\"metric\": \"gameover\", \"value\": 490}\n","loss: (min: 0.005017785355448723, avg: 0.34380863122600647, max: 5\n","{\"metric\": \"loss\", \"value\": 0.34380863122600647}\n","accuracy: (min: 0.03125, avg: 0.57285416, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5728541612625122}\n","score: (min: 0.0, avg: 1.078, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.078}\n","number of steps: (min: 120, avg: 172.934, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.934}\n","{\"metric\": \"gameover\", \"value\": 500}\n","score: (min: 0.0, avg: 1.0686274509803921, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0686274509803921}\n","number of steps: (min: 120, avg: 172.50392156862745, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.50392156862745}\n","{\"metric\": \"gameover\", \"value\": 510}\n","score: (min: 0.0, avg: 1.073076923076923, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.073076923076923}\n","number of steps: (min: 120, avg: 172.71153846153845, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.71153846153845}\n","{\"metric\": \"gameover\", \"value\": 520}\n","loss: (min: 0.004623495042324066, avg: 0.3362872199638281, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3362872199638281}\n","accuracy: (min: 0.03125, avg: 0.5762906, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5762906074523926}\n","score: (min: 0.0, avg: 1.0716981132075472, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0716981132075472}\n","number of steps: (min: 120, avg: 172.53962264150942, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.53962264150942}\n","{\"metric\": \"gameover\", \"value\": 530}\n","score: (min: 0.0, avg: 1.0703703703703704, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0703703703703704}\n","number of steps: (min: 120, avg: 172.48518518518517, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.48518518518517}\n","{\"metric\": \"gameover\", \"value\": 540}\n","loss: (min: 0.004463118966668844, avg: 0.33075932628885757, max: 5\n","{\"metric\": \"loss\", \"value\": 0.33075932628885757}\n","accuracy: (min: 0.03125, avg: 0.5801534, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5801534056663513}\n","score: (min: 0.0, avg: 1.0636363636363637, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0636363636363637}\n","number of steps: (min: 120, avg: 172.1818181818182, max: 345\n","{\"metric\": \"number of steps\", \"value\": 172.1818181818182}\n","{\"metric\": \"gameover\", \"value\": 550}\n","score: (min: 0.0, avg: 1.0571428571428572, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0571428571428572}\n","number of steps: (min: 120, avg: 171.90357142857144, max: 345\n","{\"metric\": \"number of steps\", \"value\": 171.90357142857144}\n","{\"metric\": \"gameover\", \"value\": 560}\n","loss: (min: 0.004463118966668844, avg: 0.3232674631696427, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3232674631696427}\n","accuracy: (min: 0.03125, avg: 0.58321357, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5832135677337646}\n","score: (min: 0.0, avg: 1.0596491228070175, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0596491228070175}\n","number of steps: (min: 120, avg: 171.9543859649123, max: 345\n","{\"metric\": \"number of steps\", \"value\": 171.9543859649123}\n","{\"metric\": \"gameover\", \"value\": 570}\n","score: (min: 0.0, avg: 1.0517241379310345, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0517241379310345}\n","number of steps: (min: 120, avg: 171.6155172413793, max: 345\n","{\"metric\": \"number of steps\", \"value\": 171.6155172413793}\n","{\"metric\": \"gameover\", \"value\": 580}\n","score: (min: 0.0, avg: 1.0491525423728814, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0491525423728814}\n","number of steps: (min: 120, avg: 171.4186440677966, max: 345\n","{\"metric\": \"number of steps\", \"value\": 171.4186440677966}\n","{\"metric\": \"gameover\", \"value\": 590}\n","loss: (min: 0.004463118966668844, avg: 0.3211722646230617, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3211722646230617}\n","accuracy: (min: 0.03125, avg: 0.5853053, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5853052735328674}\n","score: (min: 0.0, avg: 1.0416666666666667, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0416666666666667}\n","number of steps: (min: 120, avg: 171.16833333333332, max: 345\n","{\"metric\": \"number of steps\", \"value\": 171.16833333333332}\n","{\"metric\": \"gameover\", \"value\": 600}\n","score: (min: 0.0, avg: 1.0344262295081967, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0344262295081967}\n","number of steps: (min: 120, avg: 170.79180327868852, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.79180327868852}\n","{\"metric\": \"gameover\", \"value\": 610}\n","loss: (min: 0.004015599377453327, avg: 0.31690416562084905, max: 5\n","{\"metric\": \"loss\", \"value\": 0.31690416562084905}\n","accuracy: (min: 0.03125, avg: 0.5873259, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5873258709907532}\n","score: (min: 0.0, avg: 1.0419354838709678, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0419354838709678}\n","number of steps: (min: 120, avg: 170.9790322580645, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.9790322580645}\n","{\"metric\": \"gameover\", \"value\": 620}\n","score: (min: 0.0, avg: 1.0333333333333334, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0333333333333334}\n","number of steps: (min: 120, avg: 170.67619047619047, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.67619047619047}\n","{\"metric\": \"gameover\", \"value\": 630}\n","score: (min: 0.0, avg: 1.0359375, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0359375}\n","number of steps: (min: 120, avg: 170.765625, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.765625}\n","{\"metric\": \"gameover\", \"value\": 640}\n","loss: (min: 0.003171173157170415, avg: 0.31290068721021524, max: 5\n","{\"metric\": \"loss\", \"value\": 0.31290068721021524}\n","accuracy: (min: 0.03125, avg: 0.5894125, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.58941251039505}\n","score: (min: 0.0, avg: 1.0384615384615385, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0384615384615385}\n","number of steps: (min: 120, avg: 170.75384615384615, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.75384615384615}\n","{\"metric\": \"gameover\", \"value\": 650}\n","score: (min: 0.0, avg: 1.0333333333333334, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0333333333333334}\n","number of steps: (min: 120, avg: 170.60151515151514, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.60151515151514}\n","{\"metric\": \"gameover\", \"value\": 660}\n","loss: (min: 0.003171173157170415, avg: 0.3088631626073475, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3088631626073475}\n","accuracy: (min: 0.03125, avg: 0.59117967, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5911796689033508}\n","score: (min: 0.0, avg: 1.028358208955224, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.028358208955224}\n","number of steps: (min: 120, avg: 170.38358208955225, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.38358208955225}\n","{\"metric\": \"gameover\", \"value\": 670}\n","score: (min: 0.0, avg: 1.0235294117647058, max: 5.0\n","{\"metric\": \"score\", \"value\": 1.0235294117647058}\n","number of steps: (min: 120, avg: 170.2, max: 345\n","{\"metric\": \"number of steps\", \"value\": 170.2}\n","{\"metric\": \"gameover\", \"value\": 680}\n","score: (min: 0.0, avg: 1.0333333333333334, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0333333333333334}\n","number of steps: (min: 120, avg: 170.58115942028985, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.58115942028985}\n","{\"metric\": \"gameover\", \"value\": 690}\n","loss: (min: 0.003171173157170415, avg: 0.3076877452415989, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3076877452415989}\n","accuracy: (min: 0.03125, avg: 0.5927426, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5927426218986511}\n","score: (min: 0.0, avg: 1.0257142857142858, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0257142857142858}\n","number of steps: (min: 120, avg: 170.3242857142857, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.3242857142857}\n","{\"metric\": \"gameover\", \"value\": 700}\n","score: (min: 0.0, avg: 1.0225352112676056, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0225352112676056}\n","number of steps: (min: 120, avg: 170.18873239436618, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.18873239436618}\n","{\"metric\": \"gameover\", \"value\": 710}\n","loss: (min: 0.003171173157170415, avg: 0.30493609344274997, max: 5\n","{\"metric\": \"loss\", \"value\": 0.30493609344274997}\n","accuracy: (min: 0.03125, avg: 0.59407985, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.594079852104187}\n","score: (min: 0.0, avg: 1.0208333333333333, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0208333333333333}\n","number of steps: (min: 120, avg: 170.15277777777777, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.15277777777777}\n","{\"metric\": \"gameover\", \"value\": 720}\n","score: (min: 0.0, avg: 1.0273972602739727, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0273972602739727}\n","number of steps: (min: 120, avg: 170.4, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.4}\n","{\"metric\": \"gameover\", \"value\": 730}\n","score: (min: 0.0, avg: 1.0202702702702702, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0202702702702702}\n","number of steps: (min: 120, avg: 170.15540540540542, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.15540540540542}\n","loss: (min: 0.003171173157170415, avg: 0.30256623064878546, max: 5\n","{\"metric\": \"loss\", \"value\": 0.30256623064878546}\n","accuracy: (min: 0.03125, avg: 0.595597, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.5955970287322998}\n","{\"metric\": \"gameover\", \"value\": 740}\n","score: (min: 0.0, avg: 1.012, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.012}\n","number of steps: (min: 120, avg: 169.79733333333334, max: 372\n","{\"metric\": \"number of steps\", \"value\": 169.79733333333334}\n","{\"metric\": \"gameover\", \"value\": 750}\n","score: (min: 0.0, avg: 1.0210526315789474, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0210526315789474}\n","number of steps: (min: 120, avg: 170.07631578947368, max: 372\n","{\"metric\": \"number of steps\", \"value\": 170.07631578947368}\n","{\"metric\": \"gameover\", \"value\": 760}\n","loss: (min: 0.003171173157170415, avg: 0.3009950378670124, max: 5\n","{\"metric\": \"loss\", \"value\": 0.3009950378670124}\n","accuracy: (min: 0.03125, avg: 0.5976703, max: 1.0\n","{\"metric\": \"accuracy\", \"value\": 0.597670316696167}\n","score: (min: 0.0, avg: 1.0142857142857142, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0142857142857142}\n","number of steps: (min: 120, avg: 169.84155844155845, max: 372\n","{\"metric\": \"number of steps\", \"value\": 169.84155844155845}\n","{\"metric\": \"gameover\", \"value\": 770}\n","score: (min: 0.0, avg: 1.0192307692307692, max: 6.0\n","{\"metric\": \"score\", \"value\": 1.0192307692307692}\n","number of steps: (min: 120, avg: 169.97948717948717, max: 372\n","{\"metric\": \"number of steps\", \"value\": 169.97948717948717}\n","{\"metric\": \"gameover\", \"value\": 780}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2onyZ_jOCW6z","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}],"metadata":{"colab":{"name":"Copy of my_breakout_test.ipynb","provenance":[{"file_id":"1xOdC8gKRh4cqAs1Q8sA1MhNOIuSOPmIp","timestamp":1587480078702},{"file_id":"1alYCknot64yz2a5KJS_ssDlxjG15BX1N","timestamp":1587477209348},{"file_id":"1c3iilcbYITLeRs3Wc5fVFIj6pcr-HjH9","timestamp":1587476725648}],"collapsed_sections":[],"mount_file_id":"1alYCknot64yz2a5KJS_ssDlxjG15BX1N","authorship_tag":"ABX9TyMctr2NNZOB6y6BHQxkgvwa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}