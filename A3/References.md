- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)
In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our **dueling network** represents **two separate estimators: one for the state value function and one for the state-dependent action advantage function**. The main benefit of this factoring is to **generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm**. Our results show that this architecture leads to **better policy evaluation in the presence of many similar-valued actions**. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.

- [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)
Experience replay lets online reinforcement learning **agents remember and reuse experiences from the past**. In prior work, experience transitions were **uniformly sampled from a replay memory**. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to **replay important transitions more frequently**, and therefore learn more efficiently. We use **prioritized experience replay in Deep Q-Networks (DQN)**, a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.

- [Deep Reinforcement Learning with Double Q-learning (2015)](https://arxiv.org/abs/1509.06461)
The popular **Q-learning algorithm** is known to **overestimate action values** under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which **combines Q-learning with a deep neural network**, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in **a tabular setting**, can be generalized to work with **large-scale function approximation**. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.

- [Playing Atari with Deep Reinforcement Learning](https://researchcode.com/code/1546649608/playing-atari-with-deep-reinforcement-learning/)
We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.

- [MODEL BASED REINFORCEMENT LEARNING FOR ATARI](https://arxiv.org/pdf/1903.00374.pdf)
Model-free reinforcement learning (RL) can be used to learn effective policies
for complex tasks, such as Atari games, even from image observations. However,
this typically requires very large amounts of interaction â€“ substantially more, in
fact, than a human would need to learn the same games. How can people learn so
quickly? Part of the answer may be that people can learn how the game works and
predict which actions will lead to desirable outcomes. In this paper, we explore how
video prediction models can similarly enable agents to solve Atari games with fewer
interactions than model-free methods. We describe **Simulated Policy Learning
(SimPLe)**, a complete **model-based deep RL algorithm based on video prediction
models** and present a comparison of several model architectures, including a novel
architecture that yields the best results in our setting. Our experiments evaluate
SimPLe on a range of Atari games in low data regime of 100k interactions between
the agent and the environment, which corresponds to two hours of real-time play.
In most games SimPLe outperforms state-of-the-art model-free algorithms, in some
games by over an order of magnitude.

- [Improved robustness of reinforcement learning policies upon conversion to spiking neuronal network platforms applied to Atari Breakout game](https://www-sciencedirect-com.ezproxy.leidenuniv.nl:2443/science/article/pii/S0893608019302266?via%3Dihub)
Deep Reinforcement Learning (RL) demonstrates excellent performance on tasks that can be solved by trained policy. It plays a dominant role among cutting-edge machine learning approaches using multi-layer Neural networks (NNs). At the same time, Deep RL suffers from **high sensitivity to noisy, incomplete, and misleading input data**. Following biological intuition, we involve **Spiking Neural Networks (SNNs)** to address some deficiencies of deep RL solutions. Previous studies in image classification domain demonstrated that standard NNs (with ReLU nonlinearity) trained using supervised learning can be converted to SNNs with negligible deterioration in performance. In this paper, we extend those conversion results to the domain of Q-Learning NNs trained using RL. We provide a proof of principle of the **conversion of standard NN to SNN**. In addition, we show that the SNN has **improved robustness** to occlusion in the input image. Finally, we introduce results with converting full-scale Deep Q-network to SNN, paving the way for future research to robust Deep RL applications.
